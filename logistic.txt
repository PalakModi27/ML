import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris

iris_data = load_iris()
iris_df = pd.DataFrame(data=np.c_[iris_data['data'], iris_data['target']], columns=iris_data['feature_names']+['target'])
iris_df.head()

iris_df.loc[(iris_df["target"])>0] = 1
iris_df["target"].unique()

iris_df.isna().sum()

sns.countplot(y=iris_df.iloc[:,4], data=iris_df)
sns.heatmap(iris_df.corr(), annot=True)

X = iris_df.drop("target", axis=1)
Y = iris_df.iloc[:,4]

from sklearn.preprocessing import StandardScaler
ss = StandardScaler()
X_scaled = ss.fit_transform(X)
X_scaled=np.insert(X_scaled,0,values=1,axis=1)

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(X_scaled, Y, test_size=0.3, random_state=42)
print("x_train.shape = ", x_train.shape)
print("x_test.shape = ", x_test.shape)
print("y_train.shape = ", y_train.shape)
print("y_test.shape = ", y_test.shape)

n = 1000
alpha = 0.01
m,k = x_train.shape
beta = np.zeros(k)
for i in range(n):
    cost_gradient = np.zeros(k)
    z = x_train.dot(beta)
    predicted = 1/(1+np.exp(-z))
    difference = predicted - y_train
    for j in range(k):
        cost_gradient[j] = np.sum(difference.dot(x_train[:,j]))
    for j in range(k):
        beta[j] = beta[j] - (alpha/m)*cost_gradient[j]
print(beta)


y_pred = 1/(1+np.exp(-(x_test.dot(beta))))
y_label = np.zeros(len(y_pred))
for i in range(len(y_pred)):
    if(y_pred[i]>=0.5):
        y_label[i]=1

tp = 0
tn = 0
fp = 0
fn = 0
y_test = np.array(y_test).reshape(-1,1)
for i in range(len(y_label)):
    if(y_test[i]==1 and y_label[i]==1):
        tp+=1
    if(y_test[i]==1 and y_label[i]==0):
        fn+=1
    if(y_test[i]==0 and y_label[i]==0):
        tn+=1
    if(y_test[i]==0 and y_label[i]==1):
        fp+=1

print("True Positive = ", tp)
print("True Negative = ", tn)
print("False Positive = ", fp)
print("False Negative = ", fn)


accuracy=(tp+tn)/(tp+tn+fp+fn)
print("Accuracy = ", accuracy)

precision_pos = tp/(tp+fp)
recall_pos = tp/(tp+fn)
f1_score_pos = 2*precision_pos*recall_pos/(precision_pos+recall_pos)
print("Precision Positive = ", precision_pos)
print("Recall Positive = ", recall_pos)
print("F1_Score Positive = ", f1_score_pos)


precision_neg = tn/(tn+fn)
recall_neg = tn/(tn+fp)
f1_score_neg = 2*precision_neg*recall_neg/(precision_neg+recall_neg)
print("Precision Negative = ", precision_neg)
print("Recall Negative = ", recall_neg)
print("F1_Score Negative = ", f1_score_pos)


PLYNOMIAL(AFTER SPLITTING)


sns.scatterplot(data=exam_df, x="col1", y="col2", hue="target")

from sklearn.preprocessing import PolynomialFeatures
poly = PolynomialFeatures(degree = 6, interaction_only=False, include_bias=False)
x_train = poly.fit_transform(x_train)
x_test = poly.fit_transform(x_test)



